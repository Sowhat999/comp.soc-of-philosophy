{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import codecs\n",
    "import networkx as nx\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G = nx.read_gexf(\"influences.gexf\")\n",
    "names = G.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = codecs.open(\"stopwords.final.txt\", \"r\", \"utf-8\").read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save(name, sentences):\n",
    "  codecs.open(\"sentences/%s.txt\" % name.replace(\"/\",\"-\"), \"w\", \"utf-8\").write(\"\\n\".join(sentences))\n",
    "\n",
    "def load(name):\n",
    "  txt = codecs.open(\"pages/%s.html\" % name.replace(\"/\", \"-\"),\"r\", \"utf-8-sig\").read()\n",
    "\n",
    "  txt = BeautifulSoup(txt, \"html.parser\")\n",
    "  txt = txt.get_text()\n",
    "  \n",
    "  return txt\n",
    "\n",
    "def extract_sentences(name):\n",
    "  sentences = []\n",
    "  \n",
    "  txt = load(name)\n",
    "  \n",
    "  txt = txt.replace(\",\", \"\")\n",
    "  txt = txt.replace(\";\", \"\")\n",
    "  txt = txt.replace(\":\", \"\")\n",
    "  txt = txt.replace(\"\\\"\", \"\")\n",
    "  txt = txt.replace(\"(\", \"\")\n",
    "  txt = txt.replace(\")\", \"\")\n",
    "  txt = txt.replace(\"[edit]\", \"\")\n",
    "  txt = txt.replace(u\"â€¢\", \"\")\n",
    "\n",
    "  txt = re.sub(r'\\[[0-9]+\\]', '', txt)\n",
    "  txt = re.sub(r'ISBN [0-9\\-]+', '', txt)\n",
    "  txt = re.sub(r'(?P<a>[a-z])(?P<b>[A-Z])', '\\g<a> \\g<b>', txt)\n",
    "  txt = re.sub(r'(?P<a>[0-9])(?P<b>[A-Za-z])', '\\g<a> \\g<b>', txt)\n",
    "  \n",
    "  sentences = txt.split(\".\")\n",
    "  \n",
    "  return sentences\n",
    "\n",
    "def clean_sentences(sentences):\n",
    "  sentences = [ s.strip() for s in sentences ]\n",
    "\n",
    "  sentences = map(filter_stopwords, sentences)\n",
    "\n",
    "  # keep only sentences with at least 2 words\n",
    "  sentences = [ s for s in sentences if len(s.split()) > 1 ]\n",
    "  \n",
    "  return sentences\n",
    "\n",
    "def filter_stopwords(sentence):\n",
    "  sentence = [ w.lower() for w in sentence.split() if w.lower() not in stopwords ]\n",
    "  \n",
    "  sentence = [ w for w in sentence if w != \"\" ]\n",
    "  \n",
    "  return \" \".join(sentence)\n",
    "\n",
    "for name in names[0:50]:\n",
    "  sentences = extract_sentences(name)\n",
    "  sentences = clean_sentences(sentences)\n",
    "\n",
    "  save(name, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in names:\n",
    "  sentences = extract_sentences(name)\n",
    "  sentences = clean_sentences(sentences)\n",
    "\n",
    "  save(name, sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
